
#------------------------------------------------------------------------------
# GCP  &  Kubernetes CMDs
#------------------------------------------------------------------------------
docker ps
kubectl apply -f deployment/webapp.yml
kubectl apply -f ./my-manifest.yaml     # apply configuration to a resource by 
                                        # file or stdin. overwrides existing conf
kubectl create configmap nginx --from-file configs/lobsters.conf
kubectl create -f py.yml
kubectl create -f namespace.yml
kubectl create -f deployment/mysql.yml
kubectl create -f deployment/webapp.yml
kubectl create -f extensions/certificate.yml
kubectl create -f jobs/webapp-db-schema-load.yml
kubectl create -f jobs/webapp-db-schema-seed.yml    # get pass
kubectl create -f ./my-manifest.yaml                # Create resource(s)
kubectl create -f ./dir                      # create resources(s) in all manifest  
                                             # files in dir
kubectl create -f https://git.io/vPieio      # create from url
kubectl delete namespace <Namespace name>
kubectl describe {resource_type}             # describes the given resources
kubectl describe jobs/py                     # Example2; creating a job
kubectl describe namespace <Namespace name>
kubectl describe svc webapp                  # automaticaly, load balancer
kubectl edit svc/docker-registry        # edit the service named docker-registry
kubectl get namespace <Namespace name>
kubectl get pods
kubectl get svc
kubectl get pvc                         # storage: google cloud,persistent disk
kubectl get configmaps
kubectl get secrets                     # watch kubectl get jobs
kubectl log Testing_for_Image_pull      # gives me success or failure
kubectl logs <container name> -f 
kubectl port-f­orward {pod_name} {local­_po­rt}­:{r­emo­te_­port}      
                                        # forwart the pod port to your local port
kubectl replase --force -f ./pod.json   # force replace, delete and then 
                                        # re-create the resource
                                        # will cause a service outage
kubectl run nginx --image=nginx         # start a single instance of nginx
kubectl rolling-update {pod_name}       # perform a lling update for a pod
kubectl top                             # k8s 1.8, metrics to container CPU
kubectl svc                             # service
kubectl scale --repl­icas=3 {resou­rce­_ty­pe}­/{r­eso­urc­e_n­ame}     # scale 
watch kubectl get jobs


#------------------------------------------------------------------------------
#------------------------------------------------------------------------------
#   GCLOUD
#------------------------------------------------------------------------------
#------------------------------------------------------------------------------
gshell        # cloud shell is a machine instance that runs gcloud, my cmd line
gcloud        # connect to resources in the cloud
              # Google Cloud SDK offers permanent connection
gcloud -h
gcloud config list                           # current default project
gcloud compute --help
gcloud compute ssh <instance_name>           # ssh into instance to set up soft            # Google Kubernetes Engine (GKE); Google Compute Engine (GCE)
gcloud compute instances create --help
gcloud config set compute/zone europe-west3-c  # def value for anynew instance
gcloud compute disks create test-disk --size=50GB --zone europe-west3-c
            # creating a new persistent disk
gcloud compute instances attach-disk test-instance --disk test-disk --zone europe-west3-c
            # name of the disk; attach test disk to our test instance
            # ls -l /dev/disk/by-id/ > all hard drives (persistent-disk-1)
gcloud compute instances attach-disk test-instance --disk sdb --mode ro
            # share a non-bootable persistent dist between multiple VM,read-only
gcloud compute dist resize test-disk --size 60GB
   # sudo growpart /dev/sda1  # [PARTITION NUMBER]; run before resize2fs,3steps
   # sudo resize2fs /dev/sda/1
   # df -h


# Linux procedure for Formatting and Mounting a Persistend Disk
#------------------------------------------------------------------------------
sudo lsblk          # list attached disks to my instance
sudo mkfs.ext4 -m 0 -F -E lazy_itable_init=0, lazy_journal_init=0, discard /dev/sda
                    # format disks, make filesystem 
sudo mkdir /mnt/my-mounting-dir
sudo mount -o discard, defaults /dev/sda /mnt/my-mounting-dir
sudo chmod a+w /mnt/my-mounting-dir


#------------------------------------------------------------------------------
#------------------------------------------------------------------------------
#   KUBERNETES
#------------------------------------------------------------------------------
#------------------------------------------------------------------------------

# Resource types
#------------------------------------------------------------------------------
configmaps             # ConfigMaps allow you to decouple configuration 
                       # artifacts from image content to applications portable
deamonsets             # ensures that all or some Nodes run copy of Pod
deployments            # provides declarative updates for Pods and ReplicaSets
ingresses
jobs
namespaces
nodes
persistentvolumeclaims
persistentvolumes
pods
replicasets
cronjob
secrets                # Object is intended to hold sensitive information     
services               # ser is abstraction which defines a logical set of pods


spec:
 containers:
   - name: webapp
     image: github/version2.0.1
     env:
       -
    command:
      - "ls -la"
      - "run.sh"

# Resources limit - get app limits, we can get math, they not need to be monitor we give them contracts
resources:
  requests:
    cpu: 1
    memory: 16
  limits:
    cpu: 1
    memory: 16

# Stafefull state with volumes!  - storage is decapled from the machine
# in this scenario mount and unmount somewhere else
volumes:
  - name: mysql
  gceRersistentDisk:
    
VM 
# Openshift   - give me support, comfort
# Cloud       - Google Container Engine, Resorce Container Service
# Own Cloud   - 

disruptiveStartUp.io          - GCE
secretForeignGovtDept.xx      - 
genericWeatherService.com     -  


# Kubernetes extensions
#------------------------------------------------------------------------------
# to 
cat extensions/certificate.yml
apiVersion: extensions/v1beta1
kind: ThirdPartyResource
description: "A specification of a let's Encrypt Certificate to external https"
versions:
  - name: v1


# Kubernetes Service
#------------------------------------------------------------------------------
# service is a REST object in Kubernetes


# Kubernetes Namespace
#------------------------------------------------------------------------------
# Example5: namespace

apiVersion: v1
kind: Namespace
metadata:
  name: elasticsearch
  namespace: elk
  labels:
    com


# Kubernetes Labels & Selectors
#------------------------------------------------------------------------------
# Example4: Set-based selecors allow filtering of keys according to 
            a set of values

apiVersion :v1
kind: Service
metadata:
  name: sp-neo4j-standalone
spec:
  ports:
    - port: 7474
      name: neo4j
  type: NodePort
  selector:
    app: salesplatform
    component: neo4j


# Kubernetes Jobs
#------------------------------------------------------------------------------
# Example3 Scheduled Job "ps -eaf": every 30 min;
apiVersion: v1
kind: Job
metadata:
  name: py
spec:
  schedule: h/30 * * * *
  template:
    metadata
      name: py
    spec:
      containers:
      - name: py
        image: python
        args:
/bin/sh
-c
ps -eaf
        restartPolicy: OnFailure

# Example2: Creating a Job; template name is py; it will never restart itself

apiVersion: v1
kind: Job
metadata:
  name: py
spec:
  template:
    metadata
        name: py
    spec:
      containers:
      - name: py
        image: python
      command: ["python","SUCCESS"]
      restartPolicy: Never


# Kubernetes - Images
#------------------------------------------------------------------------------
# Each container in a pod has its Docker image running inside it
# Example1: Pull image from Docker registry and deploy in to Kubernetes conta

apiVersion: v1
kind: pod
metadata:
  name: Testing_for_Image_pull
    spec:
      containers:
      - name: neo4j-server
        image: <Name of the Docker Image>
        imagePullPolicy: Always
        command: ["echo","SUCCESS"]



# Kubernetes Setup
#------------------------------------------------------------------------------
sudo apt-get update
sudo apt-get install apt-transport-https ca-certificates
...


# Kubernetes Architecture
#------------------------------------------------------------------------------
# Kubernetes Master
      - etcd          -key value store,can be distributed among multiple nodes
      - API Server    - act as communicator among different componets of Kub
      - Scheduler     - regulates the state of cluster and performs a task
                      - run in nonterminating loop;responsible for collecting 
                        and sending information to API
    - Controller Manager - multiple kind of controllers to handle nodes

# Kubernetes Nodes  (linux machines)
      - Docker        - running the encapsulated application containers
      - Kubelet       - responsible for relaying information to and from contrl
                      - manages pods on node, volumes, secrets
                      - creating new containers etc
    - KubernetesProxy - making service available to the external host
                      - manages networking part for nodes


# Kubernetes App Deployment
#------------------------------------------------------------------------------
converting images to containers and then allocating those images to pods in
the Kubernetes cluster. setting app, deployment of app, replication cont,
replica set

# Kubernetes Autoscaling in Cluster
#------------------------------------------------------------------------------
Supporte by GCE - Google Cloud Engine
            GKE - Google Container Engine
            AWS


# Kubernetes Questions
#------------------------------------------------------------------------------
Q1: What is a Heapster?
   A1: Heapster is cluster-wide aggregator of monitring and event data. It
   supports Kubernetes natively and workds on all Kubernetes setups, including
   our Deis Workflow setup.
   (Heapster is deprecated, consider using metrics-server and third party
    metrics pipieline to gather Prometheus-format metrics instead.)
Q1-1: What is metrics-server?
    A: Starting from k9s 1.8 resource usage metrics, such as container CPU and 
    memory usage are available in Kubernetes through the Metrics API
    https://kubernetes.io/docs/tasks/debug-application-cluster/core-metrics-pipeline/
    kubectl top
Q2: What is the Kubelet?
   A2: Kubelets run pods. A pod is a collection of containers that share some
   resources: they have a single IP and can share volumes
Q3: What is Minikube?
   A3: Minikube is a tool that makes it easy to run Kubernetes locally.
   Minikube runs a single-node Kubernetes cluster inside a Vm on your laptop
   for users looking to try out Kubernetes or develop with it day-to-day
Q4: What is kubectl?
   A: kubectl is a command line interface for running commands against
   Kubernetes clusters.
Q5: What is GKE?
   A: Google Container Engine is a management and orchestration system for 
   Docker and container clusters that run within Google's publc cloud services
Q6: What is k8s?
   A: Kubenrtenes
Q7: Which process validates and configure data for the api object like pods,srv?
   A: kube-apiserver process validates and configures data for the api objects
Q8: What is the use of kube-controller-manager?
   A: Kube-controll-manager embeds the core control loop which is a
   non-terminating loop that regulates the state of the system
Q9: Kubernetes objects made up of what?
   A: Kubernetes objects are made up of Pod, Service and Volume
Q10: What are Kubernetes controllers?
   A: Kubernetes controllers are Replicaset, Deployment controller
Q11: Where Kubernetes cluster data is stored?
   A: etcd is responsible for storing KUbernetes cluster data.
Q12: What is the role of kube-scheduler?
   A: kube-scheduler is responsible for assigning a node to newly created pods
Q13: Which container runtimes supported by Kubernetes?
   A: Kubernetes supports docker and rkt container runtimes.
Q14: What are the components interact with Kubernetes node interface?
  A: Kubectl, kubelet and Node Congroller components interacts with Kubernetes
  node interface
https://codingcompiler.com/kubernetes-interview-questions-answers/


# Lift and Ship
#------------------------------------------------------------------------------
anyone tried to do Service Discovery across two or Three Kubernetes - you can't
it is not design for that. Kubernetes is design for Inter Cluster Service Discovery
# Kubernetes very basic secrets for bootstrapping things, bootstrap Kubernetes
itself, then encryption of REST and limiting what node can see

Kubernetes
    Open-shift  - Red Hat
    Lambda      - Amazon
    GKE         - Google Cloud may call it GKE plus some services

BeyondCorp. - in google, if you assume that it's going to be bad if someone is
     was insite of your network then pretend that someone is already inside your
     network and act accordingly

Kubeflow   - this is a workflow or at least an opinionated workflow for
             doing machine-learning or some analytics work

# Istio
#------------------------------------------------------------------------------
- maybe network should be smart
- Managing traffic flows between services
- Aggregating telemetry data
- Enforcing access policies
- Istio control plane (pilot, mixer, auth)
- Adaptors for backend infrastructure
- Envoy side-car

- Service Mesh

istioctl

# Vault HashCorp-
#------------------------------------------------------------------------------
Hashinetes
|  CONSUL    |    VAULT   |
|  KUBERNETES             |       NOMAD  |
|  NETWORK |  INFRASTRUCTURE |  STORAGE  |

Thise are interfases gives the way how we want to consume and compute,
some schedulers gives as row machines that we want to ssh into, some schedulers
just run the app, sometimes we don't care about machine apstraction. 

